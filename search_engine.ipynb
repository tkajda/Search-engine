{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import exists\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file parser\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "\n",
    "   \n",
    "    def filter_words(word):\n",
    "        return not (word in stop_words)\n",
    "    \n",
    "    def is_correct(word):\n",
    "        return len(word)>2 and word.isalpha()\n",
    "\n",
    "        \n",
    "    tokens = word_tokenize(text)\n",
    "    word_limit_per_article = int(1e4) \n",
    "    words_list = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "\n",
    "        if filter_words(word) and is_correct(word):\n",
    "            words_list.append(word)\n",
    "\n",
    "        if i > word_limit_per_article:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    for i, w in enumerate(words_list):\n",
    "        words_list[i] = stemmer.stem(w)\n",
    "        \n",
    "    res = \" \".join(words_list)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def parse_file(file_name, new_name):\n",
    "    res = \"\"\n",
    "    with codecs.open(file_name,'r', errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            if not line.isspace():\n",
    "                res += line\n",
    "\n",
    "    res1 = stem_text(res)\n",
    "    \n",
    "    with codecs.open(new_name, 'w+', errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        f.write(res1)\n",
    "    \n",
    "    \n",
    "\n",
    "def parse_files():\n",
    "    directory = 'DIR' \n",
    "    parsed_files = 'parsed_files/'\n",
    "    suffix = '.txt'\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        file_name = os.path.join(directory, filename)\n",
    "        new_name = parsed_files + str(i)+\"@\"+filename\n",
    "\n",
    "        if exists(file_name):\n",
    "            parse_file(file_name, new_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Od teraz pliki nie posiadją pustych linii, a słowa są 'zestemowane' oraz odfiltrowane, tak aby nie były one 'stop wordami'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def calulate_union():\n",
    "\n",
    "    directory = 'parsed_files'\n",
    "    res = dict()\n",
    "    pattern = re.compile(\"^[a-zA-Z]+$\")\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        f = os.path.join(directory, filename)\n",
    "\n",
    "        if os.path.isfile(f):\n",
    "            with open(f, 'r', errors=\"ignore\") as file:\n",
    "                for line in file.readlines():\n",
    "                    for word in line.split():\n",
    "                        if pattern.match(word):\n",
    "                            res[word] = 1\n",
    "\n",
    "    return set(res.keys())\n",
    "set_of_words = calulate_union()\n",
    "union_of_words = \" \".join(set_of_words)\n",
    "with open('union.txt', 'w+', errors=\"ignore\") as f:\n",
    "    f.write(union_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985484\n",
      "0 % done\n",
      "1 % done\n",
      "2 % done\n",
      "3 % done\n",
      "4 % done\n",
      "5 % done\n",
      "6 % done\n",
      "7 % done\n",
      "8 % done\n",
      "9 % done\n",
      "10 % done\n",
      "11 % done\n",
      "12 % done\n",
      "13 % done\n",
      "14 % done\n",
      "15 % done\n",
      "16 % done\n",
      "17 % done\n",
      "18 % done\n",
      "19 % done\n",
      "20 % done\n",
      "21 % done\n",
      "22 % done\n",
      "23 % done\n",
      "24 % done\n",
      "25 % done\n",
      "26 % done\n",
      "27 % done\n",
      "28 % done\n",
      "29 % done\n",
      "30 % done\n",
      "31 % done\n",
      "32 % done\n",
      "33 % done\n",
      "34 % done\n",
      "35 % done\n",
      "36 % done\n",
      "37 % done\n",
      "38 % done\n",
      "39 % done\n",
      "40 % done\n",
      "41 % done\n",
      "42 % done\n",
      "43 % done\n",
      "44 % done\n",
      "45 % done\n",
      "46 % done\n",
      "47 % done\n",
      "48 % done\n",
      "49 % done\n",
      "50 % done\n",
      "51 % done\n",
      "52 % done\n",
      "53 % done\n",
      "54 % done\n",
      "55 % done\n",
      "56 % done\n",
      "57 % done\n",
      "58 % done\n",
      "59 % done\n",
      "60 % done\n",
      "61 % done\n",
      "62 % done\n",
      "63 % done\n",
      "64 % done\n",
      "65 % done\n",
      "66 % done\n",
      "67 % done\n",
      "68 % done\n",
      "69 % done\n",
      "70 % done\n",
      "71 % done\n",
      "72 % done\n",
      "73 % done\n",
      "74 % done\n",
      "75 % done\n",
      "76 % done\n",
      "77 % done\n",
      "78 % done\n",
      "79 % done\n",
      "80 % done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "# class Search_Engine: \n",
    "#     def __init__(self, ):\n",
    "\n",
    "dict_of_words = {word: i for i, word in enumerate(set_of_words)}\n",
    "print(len(dict_of_words))\n",
    "\n",
    "no_files = 45362\n",
    "\n",
    "matrix = scipy.sparse.lil_matrix((len(set_of_words),no_files))\n",
    "directory = 'parsed_files'\n",
    "\n",
    "def extract_col(file_name):\n",
    "    for i in range(len(file_name)):\n",
    "        if file_name[i]==\"@\":\n",
    "            rem = i\n",
    "            break\n",
    "    \n",
    "    return int(file_name[:rem])\n",
    "\n",
    "pattern = re.compile(\"^[a-zA-Z]+$\")\n",
    "\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, \"r\",errors=\"ignore\") as file:\n",
    "            col = extract_col(filename)\n",
    "            for line in file.readlines():\n",
    "                for word in line.split():\n",
    "                    if pattern.match(word):\n",
    "                        matrix[dict_of_words[word],col] += 1\n",
    "\n",
    "    if i >int(4e4):\n",
    "        break     \n",
    "    if i % 500 == 0:\n",
    "        print((i//500), \"% done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (253, 0)\t1.0\n",
      "  (4518, 0)\t1.0\n",
      "  (5101, 0)\t1.0\n",
      "  (5351, 0)\t3.0\n",
      "  (6036, 0)\t1.0\n",
      "  (7382, 0)\t1.0\n",
      "  (8458, 0)\t1.0\n",
      "  (9099, 0)\t1.0\n",
      "  (9387, 0)\t1.0\n",
      "  (11814, 0)\t2.0\n",
      "  (12191, 0)\t1.0\n",
      "  (12407, 0)\t1.0\n",
      "  (13553, 0)\t2.0\n",
      "  (15228, 0)\t1.0\n",
      "  (18627, 0)\t3.0\n",
      "  (18776, 0)\t1.0\n",
      "  (20766, 0)\t1.0\n",
      "  (20796, 0)\t1.0\n",
      "  (21451, 0)\t19.0\n",
      "  (21534, 0)\t1.0\n",
      "  (21672, 0)\t2.0\n",
      "  (24145, 0)\t2.0\n",
      "  (27117, 0)\t1.0\n",
      "  (27308, 0)\t1.0\n",
      "  (27710, 0)\t3.0\n",
      "  :\t:\n",
      "  (963222, 0)\t1.0\n",
      "  (964257, 0)\t1.0\n",
      "  (964668, 0)\t2.0\n",
      "  (964948, 0)\t1.0\n",
      "  (966289, 0)\t2.0\n",
      "  (967846, 0)\t1.0\n",
      "  (968019, 0)\t1.0\n",
      "  (969558, 0)\t2.0\n",
      "  (971269, 0)\t1.0\n",
      "  (971820, 0)\t1.0\n",
      "  (972033, 0)\t1.0\n",
      "  (972415, 0)\t1.0\n",
      "  (973023, 0)\t1.0\n",
      "  (973127, 0)\t3.0\n",
      "  (973600, 0)\t1.0\n",
      "  (973803, 0)\t3.0\n",
      "  (975087, 0)\t1.0\n",
      "  (976447, 0)\t1.0\n",
      "  (978938, 0)\t11.0\n",
      "  (979659, 0)\t1.0\n",
      "  (980982, 0)\t1.0\n",
      "  (983377, 0)\t1.0\n",
      "  (984341, 0)\t1.0\n",
      "  (985271, 0)\t2.0\n",
      "  (985272, 0)\t3.0\n",
      "august\n"
     ]
    }
   ],
   "source": [
    "print(matrix.getcol(2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
