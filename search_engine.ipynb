{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os.path import exists\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import scipy.sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "\n",
    "\n",
    "    def __init__(self,number_of_results=10, svd_k = 100,search_svd=True, prepare_necc_files=False):\n",
    "        if prepare_necc_files:\n",
    "            self.prepare_files(svd_k)\n",
    "            \n",
    "        self.tfidf_matrix = scipy.sparse.load_npz(\"tfidf_matrix.npz\").T\n",
    "        self.vocab = self.load_vocab()\n",
    "        self.svd_matrix = np.load(\"svd_matrix.npz\")['svd_matrix']\n",
    "        self.svd_components = np.load(\"svd_comps.npz\")['svd_comps']\n",
    "        self.directory = 'parsed_files'\n",
    "        self.all_files = glob.glob(f\"{self.directory}/*\")\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.num_of_results = number_of_results\n",
    "\n",
    "        if search_svd:\n",
    "            self.search_function = self.search_with_svd\n",
    "        else: \n",
    "            self.search_function = self.search_with_no_svd\n",
    "    \n",
    "    def load_vocab(self):\n",
    "        a_file = open(\"union.pkl\", \"rb\")\n",
    "        vocab = pickle.load(a_file)\n",
    "        a_file.close()\n",
    "        return vocab\n",
    "    \n",
    "    def handle_input(self, inp):\n",
    "        \n",
    "        stemmed_words = [self.stemmer.stem(word) for word in inp.split()]\n",
    "        input_vector = np.zeros(shape=self.tfidf_matrix.shape[0])\n",
    "\n",
    "        for word in stemmed_words:\n",
    "            if self.vocab[word]:\n",
    "                input_vector[self.vocab[word]]+= 1    \n",
    "        self.search_function(input_vector)\n",
    "\n",
    "\n",
    "    def prepare_files(self, svd_k):\n",
    "        all_files = self.all_files\n",
    "\n",
    "        if not exists(\"tfidf_matrix.npz\") and not exists(\"svd_matrix.npz\") and not exists(\"union.pkl\") and not exists(\"svd_comps.npz\"):\n",
    "            \n",
    "            tfidf_vec = TfidfVectorizer(input ='filename')\n",
    "            matrix = tfidf_vec.fit_transform(all_files)\n",
    "            \n",
    "            svd = TruncatedSVD(n_components=svd_k).fit(matrix)\n",
    "            svd_matrix = svd.transform(matrix)\n",
    "            svd_components = svd.components_\n",
    "            \n",
    "            if not exists(\"tfidf_matrix.npz\"):\n",
    "                scipy.sparse.save_npz(\"tfidf_matrix\", matrix, compressed=True)\n",
    "\n",
    "            if not exists(\"union.pkl\"):\n",
    "                a_file = open(\"union.pkl\", \"wb\")\n",
    "                pickle.dump(tfidf_vec.vocabulary_, a_file)\n",
    "                a_file.close()\n",
    "            \n",
    "            if not exists(\"svd_matrix.npz\"):\n",
    "                np.savez_compressed(\"svd_matrix\", svd_matrix = svd_matrix)\n",
    "\n",
    "            if not exists(\"svd_comps.npz\"):\n",
    "                np.savez_compressed(\"svd_comps\", svd_comps=svd_components)\n",
    "\n",
    "\n",
    "    def search_with_svd(self, input_vector):\n",
    "\n",
    "        svd_q = self.svd_components @ input_vector\n",
    "        svd_c = self.svd_matrix @ svd_q\n",
    "\n",
    "        correlations = [(document_id, svd_c[document_id]) for document_id in range(len(self.all_files))]\n",
    "\n",
    "        correlations.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        for i in range(self.num_of_results):\n",
    "            print(self.all_files[correlations[i][0]])\n",
    "\n",
    "\n",
    "\n",
    "    def search_with_no_svd(self, input_vector):\n",
    "\n",
    "        sparse_x = scipy.sparse.csr_matrix(input_vector)\n",
    "\n",
    "        def vector_correlation(vec_q, vec_d):\n",
    "            norm_q = scipy.sparse.linalg.norm(vec_q)\n",
    "            norm_d = scipy.sparse.linalg.norm(vec_d)\n",
    "            return (vec_q @ vec_d) / (norm_q * norm_d)\n",
    "\n",
    "        def result_without_svd(sparse_vec):\n",
    "\n",
    "            res = []\n",
    "            for i in range(self.tfidf_matrix.shape[1]):\n",
    "                x = vector_correlation(sparse_vec, self.tfidf_matrix.getcol(i))\n",
    "                if len(x) > 0:\n",
    "                    res.append((i,x))\n",
    "\n",
    "            res.sort(key=lambda x: x[1], reverse=True)\n",
    "            for i in res[:15]:\n",
    "                print(self.all_files[i[0]])\n",
    "                \n",
    "        result_without_svd(sparse_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SearchEngine(search_svd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'united states election'\n",
    "se.handle_input(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#server\n",
    "from http.server import HTTPServer, BaseHTTPRequestHandler, SimpleHTTPRequestHandler\n",
    "\n",
    "class Serv(BaseHTTPRequestHandler):\n",
    "    \n",
    "    def _set_response(self):\n",
    "        print('set response')\n",
    "        self.send_response(200)\n",
    "        self.set_header(\"Access-Control-Allow-Origin\", \"*\")\n",
    "\n",
    "        self.send_header('Content-type', 'text/html')\n",
    "\n",
    "        self.end_headers()\n",
    "\n",
    "    def do_GET(self):\n",
    "        print('do get')\n",
    "\n",
    "        self._set_response()\n",
    "        # self.wfile.write(\"GET request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "    def do_POST(self):\n",
    "        print('do post')\n",
    "        self._set_response()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "httpd = HTTPServer(('localhost',8042),Serv)\n",
    "httpd.serve_forever()\n",
    "httpd.server_close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
