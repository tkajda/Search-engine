{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from os.path import exists\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import codecs\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import scipy.sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import tempfile\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'parsed_files'\n",
    "\n",
    "all_files = glob.glob(f\"{directory}/*\")\n",
    "tfidf_vec = TfidfVectorizer(input ='filename')\n",
    "\n",
    "matrix = tfidf_vec.fit_transform(all_files)\n",
    "# matrix_file_name = \"tfidf_matrix\"\n",
    "# scipy.sparse.save_npz(matrix_file_name, matrix, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45361, 1182713)\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = scipy.sparse.load_npz(\"tfidf_matrix.npz\")\n",
    "# tfidf_matrix = tfidf_matrix.T\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "a_file = open(\"union.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_vec.vocabulary_, a_file)\n",
    "a_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"union.pkl\", \"rb\")\n",
    "vocab = pickle.load(a_file)\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1182713, 45361)\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_matrix.T\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'footbal', 'team']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "# inp = input(\"enter keywords\")\n",
    "\n",
    "inp = 'american football team'\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in inp.split()]\n",
    "print(stemmed_words)\n",
    "\n",
    "\n",
    "x = np.zeros(shape=tfidf_matrix.shape[0])\n",
    "\n",
    "for word in stemmed_words:\n",
    "    if vocab[word]:\n",
    "        x[vocab[word]]+= 1    \n",
    "\n",
    "\n",
    "def vector_correlation(vec_q, vec_d):\n",
    "    #vec q is like a row\n",
    "    norm_q = scipy.sparse.linalg.norm(vec_q)\n",
    "    norm_d = scipy.sparse.linalg.norm(vec_d)\n",
    "\n",
    "    return (vec_q @ vec_d) / (norm_q * norm_d)\n",
    "\n",
    "sparse_x = scipy.sparse.csr_matrix(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_files\\11047@Eastern_Pacific_Ocean.txt\n",
      "parsed_files\\18458@Indian_Ocean.txt\n",
      "parsed_files\\17356@History_of_the_Union_Pacific_Railroad.txt\n",
      "parsed_files\\11121@East_Pacific_Rise.txt\n",
      "parsed_files\\1594@Atlantic.txt\n",
      "parsed_files\\1596@Atlantic_Basin.txt\n",
      "parsed_files\\1602@Atlantic_Ocean.txt\n",
      "parsed_files\\18041@Hydrographer.txt\n",
      "parsed_files\\18043@Hydrography.txt\n",
      "parsed_files\\14787@Geological_history_of_Earth.txt\n",
      "parsed_files\\10305@Division_No._1,_Saskatchewan.txt\n",
      "parsed_files\\18459@Indian_Ocean_(band).txt\n",
      "parsed_files\\19253@Isthmus_of_Panama.txt\n",
      "parsed_files\\15202@Global_surface_temperature.txt\n",
      "parsed_files\\1619@Atoll.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print( appention)\n",
    "# print(all_files[appention])\n",
    "res = []\n",
    "for i in range(tfidf_matrix.shape[1]-int(35e3)):\n",
    "    x = vector_correlation(sparse_x, tfidf_matrix.getcol(i)).data\n",
    "    if len(x) > 0:\n",
    "        res.append((i,x))\n",
    "\n",
    "        \n",
    "res.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in res[:15]:\n",
    "    print(all_files[i[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45361, 1182713)\n"
     ]
    }
   ],
   "source": [
    "print(matrix.shape)\n",
    "svd = TruncatedSVD(n_components=100).fit(matrix)\n",
    "svd_matrix = svd.transform(matrix)\n",
    "svd_components = svd.components_\n",
    "\n",
    "# self.save_svd(svd_matrix, svd_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "multi-dimensional sub-views are not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15416/3319161742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# res.sort(key = lambda x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(res.data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: multi-dimensional sub-views are not implemented"
     ]
    }
   ],
   "source": [
    "matrix_normalized = normalize(tfidf_matrix, axis=0)\n",
    "q = normalize(sparse_x)\n",
    "# print(q @ matrix_normalized)\n",
    "res = q @ svd_matrix\n",
    "# res.sort(key = lambda x)\n",
    "b = []\n",
    "for j, i in enumerate(res.data):\n",
    "    b.append((j, i))\n",
    "# print(res.data)\n",
    "b.sort(key = lambda x: x[1], reverse= True)\n",
    "# print(b)\n",
    "for i in range(15):\n",
    "    print(all_files[b[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_files\\16027@Gridiron_football.txt\n",
      "parsed_files\\74@American_football_in_the_United_States.txt\n",
      "parsed_files\\1034@Arena_football.txt\n",
      "parsed_files\\18516@Indoor_American_football.txt\n",
      "parsed_files\\67@American_football.txt\n",
      "parsed_files\\13756@Football_League_(disambiguation).txt\n",
      "parsed_files\\1469@Association_football.txt\n",
      "parsed_files\\27966@National_Football_League_(disambiguation).txt\n",
      "parsed_files\\26828@Modern_history_of_American_football.txt\n",
      "parsed_files\\13747@Football.txt\n"
     ]
    }
   ],
   "source": [
    "# q = scipy.sparse.lil_matrix((len(vocab), 1))\n",
    "# term_ids = list(set([vocab[term] for term in set(stemmer.stem(inp)) if term in vocab]))\n",
    "\n",
    "def vector_correlation(vec_q, A_k, vec_d):\n",
    "    #vec q is like a row\n",
    "    print(A_k.shape)\n",
    "    print(vec_d.shape)\n",
    "    norm_q = scipy.sparse.linalg.norm(vec_q)\n",
    "    norm_d = np.linalg.norm(A_k @ vec_d.T)\n",
    "\n",
    "    return (vec_q @ A_k @ vec_d) / (norm_q * norm_d)\n",
    "\n",
    "# res = []\n",
    "# appention = -1\n",
    "# for i in range(svd_matrix.shape[1]):\n",
    "#     x = vector_correlation(sparse_x,svd_matrix, svd_matrix[:,i])\n",
    "#     if len(x) > 0:\n",
    "#         res.append((i,x[0]))\n",
    "    \n",
    "\n",
    "svd_q = svd_components @ x\n",
    "svd_c = svd_matrix @ svd_q\n",
    "\n",
    "correlations = {document_id: svd_c[document_id] for document_id in range(len(all_files))}\n",
    "import operator\n",
    "results = sorted(correlations.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "number_of_results = 10\n",
    "for result in results[:number_of_results]:\n",
    "    print(all_files[result[0]])\n",
    "    # print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_files\\10073@Dindigul_District.txt\n",
      "parsed_files\\10047@Digitizer.txt\n",
      "parsed_files\\10002@Differential_geometry.txt\n",
      "parsed_files\\1003@Area_code_574.txt\n",
      "parsed_files\\10021@Digital_Data_Storage.txt\n",
      "parsed_files\\10059@Dilution_gene.txt\n",
      "parsed_files\\10017@Digital_content.txt\n",
      "parsed_files\\10089@Diplomacy.txt\n",
      "parsed_files\\10060@Dilwale_Dulhania_Le_Jayenge.txt\n",
      "parsed_files\\10051@Digne-les-Bains_(disambiguation).txt\n",
      "parsed_files\\10046@Digital_transformation.txt\n",
      "parsed_files\\10058@Dilly_Knox.txt\n",
      "parsed_files\\10076@Diocese#Archdiocese.txt\n",
      "parsed_files\\10040@Digital_rights_management.txt\n",
      "parsed_files\\10032@Digital_physics.txt\n"
     ]
    }
   ],
   "source": [
    "res.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in res[:15]:\n",
    "    print(all_files[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import HTTPServer, BaseHTTPRequestHandler, SimpleHTTPRequestHandler\n",
    "\n",
    "class Serv(BaseHTTPRequestHandler):\n",
    "    \n",
    "    def _set_response(self):\n",
    "        print('set response')\n",
    "        self.send_response(200)\n",
    "        self.set_header(\"Access-Control-Allow-Origin\", \"*\")\n",
    "\n",
    "        self.send_header('Content-type', 'text/html')\n",
    "\n",
    "        self.end_headers()\n",
    "\n",
    "    def do_GET(self):\n",
    "        print('do get')\n",
    "\n",
    "        self._set_response()\n",
    "        # self.wfile.write(\"GET request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "    def do_POST(self):\n",
    "        print('do post')\n",
    "        self._set_response()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "httpd = HTTPServer(('localhost',8042),Serv)\n",
    "httpd.serve_forever()\n",
    "httpd.server_close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
